{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoonShot Technologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Due to your strong performance in the first three courses, you landed a job as a reinforcement learning engineer at the hottest new non-revenue generating unicorn, MoonShot Technologies (MST). Times are busy at MST, which is preparing for its initial public offering (IPO) at the end of the fiscal year, and your labor is much needed.\n",
    "\n",
    "Like many successful startups, MST is exceedingly concerned with the valuation that it will receive at its IPO (as this valuation determines the price at which its existing venture capitalist shareholders will be able to sell their shares). Accordingly, to whet the appetites of potential investors, MST has set its sights on accomplishing a technological tour de force &mdash a lunar landing &mdash before the year is out. But it is not just any mundane lunar landing that MST aspires toward. Rather than the more sensible approach of employing techniques from aerospace engineering to pilot its spacecraft, MST endeavors to wow investors by training an agent to do the job via reinforcement learning.\n",
    "\n",
    "However, it is clearly not practical for a reinforcement learning agent to be trained tabula rasa with real rockets &mdash even the pockets of venture capitalists have their limits. Instead, MST aims to build a simulator that is realistic enough to train an agent that can be deployed in the real world. This will be a difficult project, and will require building a realistic simulator, choosing the right reinforcement learning algorithm, implementing this algorithm, and optimizing the hyperparameters for this algorithm.\n",
    "Naturally, as the newly hired reinforcement learning engineer, you have been staffed to lead the project. In this notebook, you will take the first steps by building a lunar lander environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Environment\n",
    "The software engineering team at MST has already set up some infrastructure for your convenience. Specifically they have provided you with the following functions (note - these are dummy functions just for this assignment):\n",
    "\n",
    "get_velocity() - returns an array representing the x, y velocity of the lander\n",
    "\n",
    "get_angle() - returns a scalar representing the angle of the lander\n",
    "\n",
    "get_position() - returns an array representing the x, y position of the lander\n",
    "\n",
    "get_landing_zone() - returns an array representing the x, y position of the landing zone\n",
    "\n",
    "get_fuel() - returns a scalar representing the remaining amount of fuel\n",
    "\n",
    "The main issue that you will need to consider is how to apply these methods to structure the reward function.\n",
    "\n",
    "Some things to keep in mind with the reward function:\n",
    "\n",
    "1) The lander will crash if it touches the ground when y_velocity < -3 (the downward velocity is greater than three).\n",
    "\n",
    "2) The lander will crash if it touches the ground when x_velocity < -10 or 10 < x_velocity (horizontal speed is greater than 10).\n",
    "\n",
    "3) The lander's angle taken values in [0, 360]. It is completely vertical at 0 degrees. The lander will crash if it touches the ground when 5 < angle < 355 (angle differs from vertical by more than 5 degrees).\n",
    "\n",
    "4) The lander will crash if it has yet to land and fuel <= 0 (it runs out of fuel).\n",
    "\n",
    "5) MST would like to save money on fuel when it is possible (using less fuel is preferred).\n",
    "\n",
    "6) The lander must land with it's x position within 10 of the landing zone (i.e. pos_x must be within +- land_x).\n",
    "\n",
    "Fill in the methods below to create an environment for the lunar lander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import environment\n",
    "from utils import get_landing_zone, get_angle, get_velocity, get_position, get_fuel\n",
    "get_landing_zone()\n",
    "# Lunar Lander Environment\n",
    "class LunarLanderEnvironment(environment.BaseEnvironment):\n",
    "    def __init__(self):\n",
    "        self.current_state = None\n",
    "        self.count = 0\n",
    "    \n",
    "    def env_init(self, env_info):\n",
    "        # users set this up\n",
    "        self.state = np.zeros(6) # velocity x, y, angle, distance to ground, landing zone x, y\n",
    "    \n",
    "    def env_start(self):\n",
    "        # users set this up\n",
    "        land_x, land_y = get_landing_zone()\n",
    "        self.current_state = (0, 0, 0, 100, 20, 100, land_x, land_y, 100)\n",
    "        return self.current_state\n",
    "    \n",
    "    def env_step(self, action):\n",
    "        \n",
    "        land_x, land_y = get_landing_zone()\n",
    "        vel_x, vel_y = get_velocity(action)\n",
    "        angle = get_angle(action)\n",
    "        pos_x, pos_y = get_position(action)\n",
    "        fuel = get_fuel(action)\n",
    "        \n",
    "        terminal = False\n",
    "        reward = 0.0\n",
    "        observation = (vel_x, vel_y, angle, pos_x, pos_y, land_x, land_y, fuel)\n",
    "        \n",
    "        # use the above observations to set the reward and terminal\n",
    "        # Recall - if the agent crashes or lands terminal needs to be set to True\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        if pos_y <= 0:\n",
    "            if vel_x > 10 or vel_x < -10 or vel_y < -3:\n",
    "                # crash\n",
    "                reward = -10000.0\n",
    "            elif 5 < angle < 355:\n",
    "                #crash\n",
    "                reward = -10000.0\n",
    "            else:\n",
    "                if land_x - 5 < pos_x < land_x + 5:\n",
    "                    # didn't crash small reward for remaining fuel\n",
    "                    reward = fuel\n",
    "                else:\n",
    "                    # Landed but not within the landing zone\n",
    "                    reward = 0\n",
    "            terminal = True\n",
    "        elif fuel <= 0:\n",
    "            # crash\n",
    "            reward = -10000.0\n",
    "            terminal = True\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        self.reward_obs_term = (reward, observation, terminal)\n",
    "        return self.reward_obs_term\n",
    "    \n",
    "    def env_cleanup(self):\n",
    "        return None\n",
    "    \n",
    "    def env_message(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating your reward function\n",
    "\n",
    "Designing the best reward function for an objective is a challenging task - it is not clear what the term “best reward function” even means, let alone how to find it. Consequently, rather than evaluating your reward function by quantitative metrics, we merely ask that you check that its behavior is qualitatively reasonable. For this purpose, we provide a series of test cases below. In each case we show a transition and explain how a reward function that we implemented behaves. As you read, check how your own reward behaves in each scenario and judge for yourself whether it acts appropriately. (For the latter parts of the capstone you will use our implementation of the lunar lander environment, so don’t worry if your reward function isn’t exactly the same as ours. The purpose of this of this notebook is to gain experience implementing environments and reward functions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Uncertain Future\n",
    "The lander is in the top left corner of the screen moving at a velocity of (12, 15) with 10 units of fuel &mdash whether this landing will be successful remains to be seen.\n",
    "\n",
    "![Lunar Landar](lunar_landar_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = LunarLanderEnvironment()\n",
    "ll.env_start()\n",
    "reward, obs, term = ll.env_step(1)\n",
    "print(\"Reward: {}\".format(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we gave the agent no reward, as it neither achieved the objective nor crashed. One alternative is giving the agent a positive reward for moving closer to the goal. Another is to give a negative reward for fuel consumption. What did your reward function do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Imminent Crash!\n",
    "\n",
    "The lander is positioned in the target landing zone at a 45 degree angle, but its landing gear can only handle an angular offset of five degrees &mdash it is about to crash!\n",
    "\n",
    "![Lunar Landar](lunar_landar_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = LunarLanderEnvironment()\n",
    "ll.env_start()\n",
    "reward, obs, term = ll.env_step(2)\n",
    "print(\"Reward: {}\".format(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gave the agent a reward of -10000 to punish it for crashing. How did your reward function handle the crash?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: Nice Landing!\n",
    "The lander is vertically oriented and positioned in the target landing zone with five units of remaining fuel. The landing is being completed successfully!\n",
    "\n",
    "![Lunar Landar](lunar_landar_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = LunarLanderEnvironment()\n",
    "ll.env_start()\n",
    "reward, obs, term = ll.env_step(3)\n",
    "print(\"Reward: {}\".format(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encourage the agent to conserve as much fuel as possible, we reward successful landings proportionally to the amount of fuel remaining. Here, we gave the agent a reward of five since it landed with five units of fuel remaining. How did you incentivize the agent to be fuel efficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 4: Dark Times Ahead!\n",
    "The lander is directly above the target landing zone but has no fuel left. The future does not look good for the agent &mdash without fuel there is no way for it to avoid crashing!\n",
    "\n",
    "![Lunar Landar](lunar_landar_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = LunarLanderEnvironment()\n",
    "ll.env_start()\n",
    "reward, obs, term = ll.env_step(4)\n",
    "print(\"Reward: {}\".format(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gave the agent a reward of -10000 to punish it for crashing. Did your reward function treat all crashes equally, as ours did? Or did you penalize some crashes more than others? What reasoning did you use to make this decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "Excellent! The lunar lander simulator is complete and the project can commence. In the next module, you will build upon your work here by implementing an agent to train in the environment. Don’t dally! The team at MST is eagerly awaiting your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
